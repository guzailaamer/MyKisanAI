
# MyKisanAI - Context Prompt

You are assisting me in building a production-ready MVP called **My Kisan AI** â€” a voice-first, multimodal Agentic AI assistant designed for Indian farmers. The goal is to demonstrate a working prototype within a 30-hour hackathon.

---

### ğŸ¯ Objective (What Weâ€™re Building):

The app should act as a **personal agronomist and financial advisor** for small-scale farmers in rural India. It must run as a **PWA** on basic smartphones and work in local languages.

The MVP must deliver the following core functionality:

1. **Crop Diagnosis**: Accept an image of diseased crops, analyze using Gemini Vision, and return treatment suggestions via voice/text.
2. **Market Advice**: Accept a voice query like â€œShould I sell tomatoes today?â€, fetch mandi price trends via APIs, summarize with Gemini Pro, and respond.
3. **Subsidy Guidance**: Interpret natural language questions about government schemes (e.g., â€œCan I get subsidy for drip irrigation?â€), retrieve context from embedded content (PDFs or scraped web pages), and return clear eligibility criteria and links.
4. **Voice-First Interaction**: Full STT â†’ AI â†’ TTS loop. Support Kannada, Hindi, and English.
5. **User Memory**: Store query history and user profile to personalize advice.

---

### ğŸ’¡ MVP Constraints:

- Low-bandwidth, entry-level smartphone usage
- No mobile app installation â€” must work via browser
- Google technologies only (Firebase, Vertex AI, Gemini, Firestore, Cloud Functions)
- Deliver within 30 hours

---

### ğŸ§± Architecture:

### 1. Frontend:

- Firebase Hosting (PWA)
- Vanilla HTML/CSS/JS (or React if needed)
- Upload image & record voice UI
- Fetch from backend via REST

### 2. Backend:

- Python (FastAPI) served with Uvicorn locally
- Core endpoints:
    - `/diagnose_crop` â†’ handles image input + Gemini Vision
    - `/market_advice` â†’ fetches API data + Gemini summarization
    - `/subsidy_query` â†’ RAG over docs + Gemini answer
- Modular tools in `/tools/` handle each task
- Deployed to Cloud Run or GCP Function (optional)

### 3. AI Services (All Google):

- Vertex AI Gemini Pro â†’ Language understanding
- Vertex AI Gemini Vision â†’ Image-based disease detection
- Vertex AI STT + TTS â†’ Voice-based I/O
- Firestore â†’ Stores memory: user profile, crop history, interactions

---

### ğŸ” Agentic Loop (How It Works Internally):

### ğŸ”¹ Perception:

- Accepts image (via form) or voice (via mic + STT)
- Parses user intent using Gemini Pro

### ğŸ”¹ Planning:

- Decides which tool to invoke based on parsed query
- Gathers context (e.g., location, past crop data)

### ğŸ”¹ Action:

- Calls `crop_diagnosis_tool.py`, `market_advisory_tool.py`, or `scheme_navigator_tool.py`
- Generates Gemini-powered response (voice + text)

### ğŸ”¹ Memory:

- Logs interaction in Firestore (query, timestamp, type, crop, user_id)

### ğŸ”¹ Feedback:

- Accepts thumbs up/down or correction and refines memory (not in MVP but scaffolded)

---

### ğŸ“‚ Folder Structure:

my-kisan-ai/

â”œâ”€â”€ firebase.json

â”œâ”€â”€ .firebaserc

â”œâ”€â”€ firestore.rules

â”‚

â”œâ”€â”€ frontend/

â”‚   â””â”€â”€ public/

â”‚       â””â”€â”€ index.html (PWA UI)

â”‚

â”œâ”€â”€ backend/

â”‚   â”œâ”€â”€ main.py (FastAPI app)

â”‚   â”œâ”€â”€ requirements.txt

â”‚   â””â”€â”€ tools/

â”‚       â”œâ”€â”€

**init**

.py

â”‚       â”œâ”€â”€ crop_diagnosis_tool.py     # Gemini Vision

â”‚       â”œâ”€â”€ market_advisory_tool.py    # Market API + LLM

â”‚       â”œâ”€â”€ scheme_navigator_tool.py   # RAG + Gemini

â”‚       â””â”€â”€ tts_stt_tool.py            # STT + TTS

---

### ğŸ§  Responsibilities for the Agent:

Help me build code for:

- Modular FastAPI endpoints
- Gemini API integration (Vision, Pro)
- Voice I/O with STT + TTS (Vertex AI)
- Fetch + summarize real-time market data
- Basic RAG for subsidy queries using scraped PDFs or docs
- Firestore logging of user sessions
- Feedback mechanism (scaffolded)

---

### ğŸ” Example Interaction Flow:

1. User opens PWA â†’ uploads photo or speaks in Kannada.
2. Frontend sends image or STT transcript to backend.
3. Backend identifies intent, calls relevant Gemini model/tool.
4. Backend returns advice â†’ frontend shows text + plays TTS audio.
5. Interaction is logged in Firestore with timestamp + context.

---

### ğŸ§© Tech Stack (Google Ecosystem):

| Layer | Technology |
| --- | --- |
| Frontend | Firebase Hosting, JS/HTML |
| Backend | FastAPI + Uvicorn |
| Database | Firebase Firestore |
| Hosting | Firebase / Cloud Run |
| AI Agent | Vertex AI Agent Builder (or manual loop) |
| Vision Model | Gemini Multimodal (Image) |
| Language Model | Gemini Pro |
| STT / TTS | Vertex AI Speech Services |
| Auth (optional) | Firebase Auth |

---

### âœ… Project Status:

- Firebase Hosting live
- Frontend â†’ Backend working
- FastAPI routes scaffolded
- Gemini & Vision integration pending

---

Act as a helpful co-developer that can generate:

- Backend tools
- Gemini API calls
- Firestore write/read snippets
- STT/TTS logic
- Frontend wiring
All modular and MVP-focused.